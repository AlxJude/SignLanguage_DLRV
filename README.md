# SignLanguage_DLRV

- A comparative study of the effectiveness of various deep learning architectures on the use case of signlanguage conversion to text.[2][3]
- Use case: Sign Language or hand gestures requires an interpreter at every instance. Conversation between hearing impaired people (the deaf/mute) and the non-hearing impaired people. [1][2]
-Input: Image containing a hand making the shape of an ASL letter.
-Output: ASL letter

# Requirements
- PyTorch (An open source deep learning platform)
- Tensorflow (An open source deep learning platform)

# Dataset
- The dataset consists of 1400 images which are 720x720 pixels. There are 26 classes for the letters A-Z.

# Models
The model informations and the evaluations are given in the DLRV_PresentationSlides.

# Table Of Contents
-  [In a Nutshell](#in-a-nutshell)
-  [Future Work](#future-work)
-  [References](#references)

# How to use code
Each of the .ipynb files were created and run in their separate conda environments.use the requirements.txt file to install dependencies.

# In a Nutshell
```
├──  ChangeDimentions
│    └── ChangeDimentions  - here's the python file to change the dimensions of the images.
│
├──  Code1  - Directory for training the sign language model using the CNN based architecture.
│    └── requirements.txt  - here's the specific config file for all the dependencies of the model.
│    └── Code1Train.ipynb  - here's the ipython file for training the model.
│    └── Code1_LiveTest.ipynb  - here's the ipython file for live inference using the model.
│
├──  Code2  - Fine tuning pre-trained VGG16 model.
│    └── requirements.txt  - here's the specific config file for all the dependencies of the model.
│    └── Code2.ipynb  - here's the ipython file for inference using the fined tuned VGG model.
│
├──  CollectedImages - this folder contains images generated by us
│
├── Data              - this folder contains the data from the kaggle dataset.
│
└── ImageCollection            - this folder contains the python file to capture the images for the dataset.
   └── ImageCollection.ipynb  - here's the ipython file for capturing the gesture images for creating the dataset.
```

# Image Collection (Optional)
- Run ’ImageCollection.ipynb’ to collect images.
- This program will initialte the camera to click training images.
- The images will be stored in ’CollectedImages’ folder with each alphabet having its own folder.
- Vary SAMPLES to change the number of images needed to be collected.
- The images are already collected and can be used directly.

# Change Image Dimensions (Optional)
- Run ’ChangeDimension.ipynb’ to change the dimensions of the images stored in ’CollectedImages’folder.
- The collected images are not in the desired dimensions (28 x 28) and therefore need to to be converted to this required format.
- The output of this program are 2 csv files that store the train and test data-sets.

# Code 1
- Run ’Code1Train.ipynb’[4] file to train the CNN based (model built from scratch).
- To switch between datasets use the USEMNISTDATASET Boolean constant. True to use MNISTdata-set and false to use user collected data-set.
- To test the trained model on a live feed run Code1LiveTest.ipynb’ file.
-T his program turns the camera on as a live feed and predicts in real time the alphabets of the hand signs.

# Code 2
- Code 2[3][4] folder contains code for inference part of the trained pre-trained VGG16 model.
- Install requirements using the requirements.txt file.
- Run ’Code2.ipynb’4to load the pre-trained model and perform real time sign language to alphabet prediction.

# Future Work
- Adding the region proposal network in live sign language prediction

# References
1. M. Taskiran, M. Killioglu and N. Kahraman, ”A Real-Time System for Recognition of American Sign Language by usingDeep Learning,” 2018 41st International Conference on Telecommunications and Signal Processing (TSP), 2018, pp. 1-5, doi:10.1109/TSP.2018.8441304.3S.
2. Sharma, H. Pallab Jyoti Dutta, M. K. Bhuyan and R. H. Laskar, ”Hand Gesture Localization and Classification byDeep Neural Network for Online Text Entry,” 2020 IEEE Applied Signal Processing Conference (ASPCON), 2020, pp. 298-302,doi: 10.1109/ASPCON49795.2020.9276713
3. https://www.digitalocean.com/community/tutorials/how-to-build-a-neural-network-to-translate-sign-language-into-english
4. https://data-flair.training/blogs/sign-language-recognition-python-ml-opencv/
