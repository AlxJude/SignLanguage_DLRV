{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##PROJECT GUIDELINE: https://data-flair.training/blogs/sign-language-recognition-python-ml-opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb782246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Input, Lambda ,Dense ,Flatten ,Dropout\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import np_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "path= os.path.expanduser(\"model_cnn.h5\")\n",
    "print(path)\n",
    "model = load_model(r\"model_cnn.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72082973",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import imutils\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "video_capture = cv2.VideoCapture(1)\n",
    "video_capture1 = cv2.VideoCapture(1)\n",
    "\n",
    "uniq_labels = ['A',\n",
    " 'B',\n",
    " 'C',\n",
    " 'D',\n",
    " 'E',\n",
    " 'F',\n",
    " 'G',\n",
    " 'H',\n",
    " 'I',\n",
    " 'J',\n",
    " 'K',\n",
    " 'L',\n",
    " 'M',\n",
    " 'N',\n",
    " 'O',\n",
    " 'P',\n",
    " 'Q',\n",
    " 'R',\n",
    " 'S',\n",
    " 'T',\n",
    " 'U',\n",
    " 'V',\n",
    " 'W',\n",
    " 'X',\n",
    " 'Y',\n",
    " 'Z']\n",
    " \n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frames = video_capture.read()\n",
    "    frames2= cv2.flip(frames,1)\n",
    "    frames = cv2.resize(frames, dsize=(64,64))\n",
    "    #frames = cv2.flip(frames,1)\n",
    "    #frames = cv2.cvtColor(frames, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    #cv2.imshow('Title', frames)\n",
    "    # print(frames.shape)\n",
    "    test_image = image.img_to_array(frames)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "    # test_image = cv2.resize(test_image, dsize=(64,64))\n",
    "\n",
    "    result = model.predict(test_image)\n",
    "    result = np.argmax(result)\n",
    "    # print(result)\n",
    "    \n",
    "    #print(uniq_labels[result])\n",
    "    cv2.putText(frames2, uniq_labels[result], (100,200), cv2.FONT_HERSHEY_SIMPLEX, 5.0, (0, 255, 0), thickness=5)\n",
    "    cv2.imshow('Sign Language To Text', frames2)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
